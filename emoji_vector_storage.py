#!/usr/bin/env python3
"""
Emoji Vector Storage - Store emoji emotion data in Azure AI Search with embeddings

This script reads emoji emotion data from JSON and stores it in Azure AI Search
with vector embeddings generated by Azure OpenAI.
"""

import os
import json
import time
import logging
from typing import Dict, List, Optional, Any
from pathlib import Path
from dataclasses import dataclass
from concurrent.futures import ThreadPoolExecutor, as_completed

try:
    from azure.search.documents import SearchClient
    from azure.search.documents.indexes import SearchIndexClient
    from azure.search.documents.models import VectorizedQuery
    from azure.search.documents.indexes.models import (
        SearchIndex,
        SearchField,
        SearchFieldDataType,
        SimpleField,
        SearchableField,
        VectorSearch,
        VectorSearchProfile,
        VectorSearchAlgorithmKind,
        VectorSearchAlgorithmMetric,
        HnswAlgorithmConfiguration
    )
    from azure.core.credentials import AzureKeyCredential
    from azure.identity import DefaultAzureCredential
    AZURE_SEARCH_AVAILABLE = True
except ImportError:
    AZURE_SEARCH_AVAILABLE = False
    print("⚠️ Azure Search libraries not available. Install with: pip install azure-search-documents azure-identity")

try:
    from openai import AzureOpenAI
    AZURE_OPENAI_AVAILABLE = True
except ImportError:
    AZURE_OPENAI_AVAILABLE = False
    print("⚠️ OpenAI library not available. Install with: pip install openai")


@dataclass
class EmojiDocument:
    """Document structure for emoji data in vector database"""
    id: str
    emoji_name: str
    filename: str
    primary_emotion: str
    secondary_emotions: str
    usage_scenarios: str
    tone: str
    context_suggestions: str
    content_vector: List[float]
    searchable_text: str


class EmojiVectorStorage:
    """Store emoji emotion data in Azure AI Search with embeddings"""

    def __init__(self, config_file: str = "azure_config.json"):
        """Initialize with Azure configuration"""
        self.config = self._load_config(config_file)
        self.logger = self._setup_logging()
        
        # Azure AI Search configuration
        self.search_service_name = self.config.get('search_service_name')
        self.search_admin_key = self.config.get('search_admin_key')
        self.search_endpoint = f"https://{self.search_service_name}.search.windows.net"
        self.index_name = self.config.get('emoji_index_name', 'emoji-emotions')
        
        # Azure OpenAI configuration for embeddings
        self.openai_endpoint = self.config.get('azure_openai_endpoint')
        self.openai_api_key = self.config.get('azure_openai_api_key')
        self.openai_api_version = self.config.get('azure_openai_api_version', '2024-02-15-preview')
        self.embedding_deployment = self.config.get('embedding_deployment_name', 'text-embedding-ada-002')
        
        # Validate configuration
        self._validate_config()
        
        # Initialize clients
        self._initialize_clients()
        
    def _load_config(self, config_file: str) -> Dict:
        """Load configuration from JSON file"""
        try:
            if os.path.exists(config_file):
                with open(config_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            else:
                return {}
        except Exception as e:
            print(f"⚠️ Warning: Could not load config file {config_file}: {e}")
            return {}
    
    def _setup_logging(self) -> logging.Logger:
        """Setup logging configuration"""
        logging.basicConfig(
            level=logging.DEBUG,  # Set to DEBUG for more detailed output
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        return logging.getLogger(__name__)
    
    def _validate_config(self):
        """Validate required configuration parameters"""
        required_configs = {
            'search_service_name': 'Azure AI Search service name',
            'search_admin_key': 'Azure AI Search admin key',
            'azure_openai_endpoint': 'Azure OpenAI endpoint',
            'azure_openai_api_key': 'Azure OpenAI API key'
        }
        
        for config_key, description in required_configs.items():
            if not self.config.get(config_key):
                raise ValueError(f"Missing required configuration: {description}")
    
    def _initialize_clients(self):
        """Initialize Azure clients with proper authentication"""
        try:
            # Use managed identity in production, API key for development
            if self.search_admin_key and self.search_admin_key != "your-search-admin-key-here":
                search_credential = AzureKeyCredential(self.search_admin_key)
            else:
                search_credential = DefaultAzureCredential()
            
            # Initialize Azure AI Search clients
            self.search_index_client = SearchIndexClient(
                endpoint=self.search_endpoint,
                credential=search_credential
            )
            
            self.search_client = SearchClient(
                endpoint=self.search_endpoint,
                index_name=self.index_name,
                credential=search_credential
            )
            
            # Initialize Azure OpenAI client
            self.openai_client = AzureOpenAI(
                api_key=self.openai_api_key,
                api_version=self.openai_api_version,
                azure_endpoint=self.openai_endpoint
            )
            
            self.logger.info("✅ Azure clients initialized successfully")
            
        except Exception as e:
            self.logger.error(f"❌ Failed to initialize Azure clients: {e}")
            raise
    
    def create_search_index(self) -> bool:
        """Create the search index with vector search configuration"""
        try:
            # Check if index already exists
            try:
                existing_index = self.search_index_client.get_index(self.index_name)
                self.logger.info(f"✅ Search index '{self.index_name}' already exists")
                return True
            except Exception:
                # Index doesn't exist, create it
                pass
            
            # Define vector search configuration without automatic vectorizer
            vector_search = VectorSearch(
                    algorithms=[
                        HnswAlgorithmConfiguration(
                            name="hnsw-algorithm",
                            kind=VectorSearchAlgorithmKind.HNSW,
                            parameters={
                                "m": 4,
                                "efConstruction": 400,
                                "efSearch": 500,
                                "metric": VectorSearchAlgorithmMetric.COSINE
                            }
                        )
                    ],
                profiles=[
                    VectorSearchProfile(
                        name="emoji-vector-profile",
                        algorithm_configuration_name="hnsw-algorithm"
                    )
                ]
            )
            
            # Define search fields with proper configuration
            fields = [
                SimpleField(name="id", type=SearchFieldDataType.String, key=True),
                SearchableField(name="emoji_name", type=SearchFieldDataType.String),
                    SimpleField(name="filename", type=SearchFieldDataType.String, filterable=True),
                SearchableField(name="primary_emotion", type=SearchFieldDataType.String, facetable=True),
                SearchableField(name="secondary_emotions", type=SearchFieldDataType.String, facetable=True),
                SearchableField(name="usage_scenarios", type=SearchFieldDataType.String),
                SearchableField(name="tone", type=SearchFieldDataType.String, facetable=True),
                SearchableField(name="context_suggestions", type=SearchFieldDataType.String),
                SearchableField(name="searchable_text", type=SearchFieldDataType.String),
                SearchField(
                    name="content_vector",
                    type=SearchFieldDataType.Collection(SearchFieldDataType.Single),
                    vector_search_dimensions=1536,  # text-embedding-3-small dimension
                    vector_search_profile_name="emoji-vector-profile"
                )
            ]
            
            # Create the index
            index = SearchIndex(
                name=self.index_name,
                fields=fields,
                vector_search=vector_search
            )
            
            self.logger.info(f"🔨 Creating search index '{self.index_name}'...")
            result = self.search_index_client.create_or_update_index(index)
            self.logger.info(f"✅ Search index '{self.index_name}' created successfully")
            
            # Wait a moment for the index to be ready
            time.sleep(2)
            return True
            
        except Exception as e:
            self.logger.error(f"❌ Failed to create search index: {e}")
            return False
    
    def generate_embedding(self, text: str, max_retries: int = 3) -> Optional[List[float]]:
        """Generate embedding for text using Azure OpenAI"""
        for attempt in range(max_retries):
            try:
                response = self.openai_client.embeddings.create(
                    model=self.embedding_deployment,
                    input=text
                )
                return response.data[0].embedding
                
            except Exception as e:
                if attempt < max_retries - 1:
                    wait_time = 2 ** attempt
                    self.logger.warning(f"⚠️ Embedding generation failed (attempt {attempt + 1}), retrying in {wait_time}s: {e}")
                    time.sleep(wait_time)
                else:
                    self.logger.error(f"❌ Failed to generate embedding after {max_retries} attempts: {e}")
                    return None
    
    @staticmethod
    def convert_to_string(data, logger=None, context_key: str = None) -> str:
        """Convert any data structure to a string"""
        if data is None:
            return ''
        if isinstance(data, str):
            return data.strip()
        if isinstance(data, (int, float, bool)):
            return str(data)
        if isinstance(data, list):
            # Convert list to comma-separated string
            string_items = []
            for item in data:
                if isinstance(item, str):
                    if item.strip():  # Only add non-empty strings
                        string_items.append(item.strip())
                elif isinstance(item, (int, float, bool)):
                    string_items.append(str(item))
                elif isinstance(item, (list, dict)):
                    # Log warning for nested structures and convert to string
                    if logger and context_key:
                        logger.warning(f"⚠️ Found nested structure in {context_key}: {type(item)}")
                    string_items.append(str(item))
                else:
                    string_items.append(str(item))
            return ', '.join(string_items)
        elif isinstance(data, dict):
            # Convert dict to a string representation
            if logger and context_key:
                logger.warning(f"⚠️ Found dict in field for {context_key}")
            return str(data)
        else:
            return str(data)

    def sanitize_document_id(self, emoji_key: str) -> str:
        """Sanitize document ID to comply with Azure AI Search requirements"""
        import re
        
        # Replace spaces and other invalid characters with underscores
        # Azure AI Search allows: letters, digits, underscore (_), dash (-), or equal sign (=)
        sanitized = re.sub(r'[^a-zA-Z0-9_\-=]', '_', emoji_key)
        
        # Remove multiple consecutive underscores
        sanitized = re.sub(r'_+', '_', sanitized)
        
        # Remove leading/trailing underscores
        sanitized = sanitized.strip('_')
        
        # Ensure it's not empty
        if not sanitized:
            sanitized = 'emoji_unknown'
            
        return sanitized

    def prepare_document(self, emoji_key: str, emoji_data: Dict) -> Optional[EmojiDocument]:
        """Prepare emoji data for vector database storage"""
        try:
            # Sanitize the document ID to comply with Azure AI Search requirements
            sanitized_id = self.sanitize_document_id(emoji_key)
            
            # Safely extract data with proper type checking and defaults
            emoji_name = emoji_data.get('emoji_name', '')
            primary_emotion = emoji_data.get('primary_emotion', 'neutral')  # Default to neutral if missing
            secondary_emotions_raw = emoji_data.get('secondary_emotions', '')
            usage_scenarios_raw = emoji_data.get('usage_scenarios', '')
            tone = emoji_data.get('tone', 'neutral')  # Default to neutral if missing
            context_suggestions_raw = emoji_data.get('context_suggestions', '')
            filename = emoji_data.get('filename', '')
            
            # Convert all fields to strings using the static method
            secondary_emotions_str = self.convert_to_string(secondary_emotions_raw, self.logger, emoji_key)
            usage_scenarios_str = self.convert_to_string(usage_scenarios_raw, self.logger, emoji_key)
            context_suggestions_str = self.convert_to_string(context_suggestions_raw, self.logger, emoji_key)
            
            # Use only context_suggestions for the content vector
            # This focuses the embedding on the practical use cases of the emoji
            searchable_text = ':'.join([
                emoji_name,
                primary_emotion,
                secondary_emotions_str,
                tone,
                usage_scenarios_str,
                context_suggestions_str
            ])
            
            # Ensure we have some text for embedding - use emoji name as fallback
            if not searchable_text or len(searchable_text.strip()) == 0:
                searchable_text = f"{emoji_name} emoji"
                self.logger.warning(f"⚠️ Using fallback searchable text for {emoji_key}: '{searchable_text}'")
            
            # Generate embedding for the searchable text
            embedding = self.generate_embedding(searchable_text)
            if not embedding:
                self.logger.warning(f"⚠️ Failed to generate embedding for {emoji_key}")
                return None
            
            # Create document with validated data
            document = EmojiDocument(
                id=sanitized_id,  # Use sanitized ID
                emoji_name=emoji_name,
                filename=filename,
                primary_emotion=primary_emotion,
                secondary_emotions=secondary_emotions_str,
                usage_scenarios=usage_scenarios_str,
                tone=tone,
                context_suggestions=context_suggestions_str,
                content_vector=embedding,
                searchable_text=searchable_text
            )
            
            # Log ID sanitization if changed
            if sanitized_id != emoji_key:
                self.logger.info(f"🔧 Sanitized document ID: '{emoji_key}' -> '{sanitized_id}'")
            
            return document
            
        except Exception as e:
            self.logger.error(f"❌ Failed to prepare document for {emoji_key}: {e}")
            return None
            
        except Exception as e:
            self.logger.error(f"❌ Failed to prepare document for {emoji_key}: {e}")
            return None
    
    def validate_document_structure(self, doc_dict: Dict) -> bool:
        """Validate document structure before upload"""
        try:
            doc_id = doc_dict.get('id', 'unknown')
            required_fields = ['id', 'emoji_name', 'primary_emotion', 'content_vector']
            
            # Check required fields
            for field in required_fields:
                if field not in doc_dict:
                    self.logger.error(f"❌ {doc_id}: Missing required field: {field}")
                    return False
            
            # Validate field types
            if not isinstance(doc_dict['id'], str) or not doc_dict['id'].strip():
                self.logger.error(f"❌ {doc_id}: Invalid id field: must be non-empty string")
                return False
                
            if not isinstance(doc_dict['content_vector'], list):
                self.logger.error(f"❌ {doc_id}: content_vector must be a list, got: {type(doc_dict['content_vector'])}")
                return False
                
            if len(doc_dict['content_vector']) != 1536:
                self.logger.error(f"❌ {doc_id}: content_vector must have 1536 dimensions, got: {len(doc_dict['content_vector'])}")
                return False
                
            # Check that all vector elements are numeric and convert to float
            try:
                doc_dict['content_vector'] = [float(val) for val in doc_dict['content_vector']]
            except (ValueError, TypeError) as e:
                self.logger.error(f"❌ {doc_id}: content_vector contains non-numeric values: {e}")
                return False
            
            # Validate and clean string fields that may contain structured data
            structured_string_fields = ['secondary_emotions', 'usage_scenarios', 'context_suggestions']
            for field in structured_string_fields:
                if field in doc_dict:
                    if doc_dict[field] is None:
                        doc_dict[field] = ''
                    elif not isinstance(doc_dict[field], str):
                        self.logger.warning(f"⚠️ {doc_id}: Converting {field} from {type(doc_dict[field])} to string")
                        doc_dict[field] = str(doc_dict[field])
                else:
                    # Ensure field exists even if not in original document
                    doc_dict[field] = ''
            
            # Validate and clean string fields
            string_fields = ['emoji_name', 'filename', 'primary_emotion', 'tone', 'searchable_text']
            for field in string_fields:
                if field in doc_dict:
                    if doc_dict[field] is None:
                        doc_dict[field] = ''
                    elif not isinstance(doc_dict[field], str):
                        self.logger.warning(f"⚠️ {doc_id}: Converting {field} from {type(doc_dict[field])} to string")
                        doc_dict[field] = str(doc_dict[field])
                else:
                    # Ensure field exists even if not in original document
                    doc_dict[field] = ''
            
            # Final validation - log the document structure for debugging problematic documents
            if doc_id == 'anatomicalheart':
                self.logger.debug(f"🔍 Detailed structure for {doc_id}:")
                for key, value in doc_dict.items():
                    if key == 'content_vector':
                        self.logger.debug(f"  {key}: list of {len(value)} floats")
                    elif isinstance(value, str):
                        self.logger.debug(f"  {key}: str = '{value}'")
                    else:
                        self.logger.debug(f"  {key}: {type(value).__name__} = '{value}'")
            
            return True
            
        except Exception as e:
            self.logger.error(f"❌ Error validating document structure for {doc_dict.get('id', 'unknown')}: {e}")
            return False

    def upload_documents(self, documents: List[EmojiDocument], batch_size: int = 10) -> bool:
        """Upload documents to Azure AI Search in batches"""
        try:
            # Convert documents to dictionaries with proper validation
            doc_dicts = []
            for doc in documents:
                # Ensure all fields are properly initialized and typed
                # Handle None values and ensure proper data types
                doc_dict = {
                    'id': str(doc.id) if doc.id else '',
                    'emoji_name': str(doc.emoji_name) if doc.emoji_name else '',
                    'filename': str(doc.filename) if doc.filename else '',
                    'primary_emotion': str(doc.primary_emotion) if doc.primary_emotion else '',
                    'secondary_emotions': '',
                    'usage_scenarios': '',
                    'tone': str(doc.tone) if doc.tone else '',
                    'context_suggestions': '',
                    'content_vector': [],
                    'searchable_text': str(doc.searchable_text) if doc.searchable_text else ''
                }
                
                # Handle string fields - ensure they are strings
                if hasattr(doc, 'secondary_emotions') and doc.secondary_emotions is not None:
                    doc_dict['secondary_emotions'] = str(doc.secondary_emotions)
                
                # Handle string fields - ensure they are strings
                if hasattr(doc, 'usage_scenarios') and doc.usage_scenarios is not None:
                    doc_dict['usage_scenarios'] = str(doc.usage_scenarios)
                
                # Handle string fields - ensure they are strings
                if hasattr(doc, 'context_suggestions') and doc.context_suggestions is not None:
                    doc_dict['context_suggestions'] = str(doc.context_suggestions)
                
                # Handle content_vector - ensure it's a proper float array
                if hasattr(doc, 'content_vector') and doc.content_vector is not None:
                    if isinstance(doc.content_vector, list):
                        try:
                            doc_dict['content_vector'] = [float(x) for x in doc.content_vector]
                        except (ValueError, TypeError) as e:
                            self.logger.error(f"❌ {doc.id}: Invalid content_vector data: {e}")
                            continue
                    else:
                        self.logger.error(f"❌ {doc.id}: content_vector must be a list")
                        continue
                else:
                    self.logger.error(f"❌ {doc.id}: Missing content_vector")
                    continue
                
                # Validate document structure before adding to batch
                if self.validate_document_structure(doc_dict):
                    doc_dicts.append(doc_dict)
                else:
                    self.logger.warning(f"⚠️ Skipping invalid document: {doc.id}")
                    continue
            
            if not doc_dicts:
                self.logger.error("❌ No valid documents to upload")
                return False
            
            # Upload in batches with smaller batch size for better error isolation
            batch_size = min(batch_size, 5)  # Reduce batch size for debugging
            total_successful = 0
            
            for i in range(0, len(doc_dicts), batch_size):
                batch = doc_dicts[i:i + batch_size]
                batch_num = i//batch_size + 1
                
                self.logger.info(f"📤 Processing batch {batch_num} with {len(batch)} documents")
                
                try:
                    # Debug: Log the exact JSON being sent for the first document
                    if i == 0:
                        import json as json_module
                        sample_doc = batch[0] if batch else {}
                        self.logger.debug(f"🔍 Sample JSON being sent to Azure AI Search:")
                        self.logger.debug(json_module.dumps(sample_doc, indent=2, default=str)[:1000] + "...")
                    
                    result = self.search_client.upload_documents(documents=batch)
                    
                    success_count = sum(1 for r in result if r.succeeded)
                    failed_count = len(batch) - success_count
                    total_successful += success_count
                    
                    if failed_count > 0:
                        # Log specific failures
                        for r in result:
                            if not r.succeeded:
                                self.logger.error(f"❌ Failed to upload document {r.key}: {r.error_message}")
                    
                    self.logger.info(f"✅ Batch {batch_num}: {success_count}/{len(batch)} documents uploaded successfully")
                    
                except Exception as batch_error:
                    self.logger.error(f"❌ Failed to upload batch {batch_num}: {batch_error}")
                    
                    # Try uploading documents one by one to identify the problematic document
                    self.logger.info(f"🔍 Attempting individual uploads for batch {batch_num}...")
                    for idx, doc in enumerate(batch):
                        try:
                            doc_id = doc.get('id', f'unknown_{idx}')
                            
                            # Additional validation before individual upload
                            if not self.validate_document_structure(doc):
                                self.logger.error(f"❌ Document {doc_id} failed validation, skipping individual upload")
                                continue
                            
                            single_result = self.search_client.upload_documents(documents=[doc])
                            
                            if single_result[0].succeeded:
                                self.logger.info(f"✅ Individual upload successful: {doc_id}")
                                total_successful += 1
                            else:
                                self.logger.error(f"❌ Individual upload failed for {doc_id}: {single_result[0].error_message}")
                                
                        except Exception as single_error:
                            self.logger.error(f"❌ Individual upload error for {doc_id}: {single_error}")
                    
                    continue
                
                # Rate limiting
                time.sleep(1)
            
            self.logger.info(f"✅ Upload completed: {total_successful}/{len(doc_dicts)} documents uploaded successfully")
            return total_successful > 0
            
        except Exception as e:
            self.logger.error(f"❌ Failed to upload documents: {e}")
            return False
    
    def search_similar_emojis(self, query: str, top_k: int = 5) -> List[Dict]:
        """Search for similar emojis using vector similarity"""
        try:
            # Generate query embedding
            query_embedding = self.generate_embedding(query)
            if not query_embedding:
                return []
            
            # Create vector query
            vector_query = VectorizedQuery(
                vector=query_embedding,
                k_nearest_neighbors=top_k,
                fields="content_vector"
            )
            
            # Perform search
            results = self.search_client.search(
                search_text=None,
                vector_queries=[vector_query],
                select=["id", "emoji_name", "primary_emotion", "secondary_emotions", "usage_scenarios", "tone"],
                top=top_k
            )
            
            # Format results
            formatted_results = []
            for result in results:
                formatted_results.append({
                    'id': result['id'],
                    'emoji_name': result['emoji_name'],
                    'primary_emotion': result['primary_emotion'],
                    'secondary_emotions': result['secondary_emotions'],
                    'usage_scenarios': result['usage_scenarios'],
                    'tone': result['tone'],
                    'score': result['@search.score']
                })
            
            return formatted_results
            
        except Exception as e:
            self.logger.error(f"❌ Search failed: {e}")
            return []
    
    def delete_emoji_by_filename(self, filename: str) -> bool:
        """Delete emoji from vector database by filename"""
        try:
            # First, search for the emoji to get its document ID
            search_results = self.search_client.search(
                search_text="*",
                filter=f"filename eq '{filename}'",
                select=["id", "filename", "emoji_name"],
                top=1
            )
            
            # Check if document exists
            document_to_delete = None
            for result in search_results:
                document_to_delete = result
                break
            
            if not document_to_delete:
                self.logger.warning(f"⚠️ No emoji found with filename: {filename}")
                return False
            
            # Delete the document from the search index
            document_id = document_to_delete["id"]
            self.logger.info(f"🗑️ Deleting emoji: {filename} (ID: {document_id})")
            
            # Prepare delete action
            delete_actions = [
                {
                    "@search.action": "delete",
                    "id": document_id
                }
            ]
            
            # Execute delete
            result = self.search_client.upload_documents(documents=delete_actions)
            
            # Check if delete was successful
            if result and len(result) > 0:
                success = result[0].succeeded
                if success:
                    self.logger.info(f"✅ Successfully deleted emoji from database: {filename}")
                    return True
                else:
                    self.logger.error(f"❌ Failed to delete emoji from database: {filename}")
                    return False
            else:
                self.logger.error(f"❌ No result returned from delete operation for: {filename}")
                return False
                
        except Exception as e:
            self.logger.error(f"❌ Error deleting emoji {filename}: {e}")
            return False
    
    def delete_emoji_by_id(self, document_id: str) -> bool:
        """Delete emoji from vector database by document ID"""
        try:
            self.logger.info(f"🗑️ Deleting emoji by ID: {document_id}")
            
            # Prepare delete action
            delete_actions = [
                {
                    "@search.action": "delete",
                    "id": document_id
                }
            ]
            
            # Execute delete
            result = self.search_client.upload_documents(documents=delete_actions)
            
            # Check if delete was successful
            if result and len(result) > 0:
                success = result[0].succeeded
                if success:
                    self.logger.info(f"✅ Successfully deleted emoji from database (ID: {document_id})")
                    return True
                else:
                    self.logger.error(f"❌ Failed to delete emoji from database (ID: {document_id})")
                    return False
            else:
                self.logger.error(f"❌ No result returned from delete operation (ID: {document_id})")
                return False
                
        except Exception as e:
            self.logger.error(f"❌ Error deleting emoji by ID {document_id}: {e}")
            return False

    def analyze_emoji_data(self, json_file: str) -> Dict:
        """Analyze emoji data to identify potential issues"""
        try:
            with open(json_file, 'r', encoding='utf-8') as f:
                emoji_data = json.load(f)
            
            analysis = {
                'total_entries': len(emoji_data),
                'complete_entries': 0,
                'incomplete_entries': 0,
                'missing_fields': {},
                'empty_fields': {},
                'problematic_entries': []
            }
            
            required_fields = ['emoji_name', 'primary_emotion', 'filename']
            optional_fields = ['secondary_emotions', 'usage_scenarios', 'tone', 'context_suggestions']
            
            for key, data in emoji_data.items():
                is_complete = True
                missing_fields = []
                empty_fields = []
                
                # Check required fields
                for field in required_fields:
                    if field not in data:
                        missing_fields.append(field)
                        is_complete = False
                    elif not data[field] or (isinstance(data[field], str) and not data[field].strip()):
                        empty_fields.append(field)
                        is_complete = False
                
                # Check optional fields
                for field in optional_fields:
                    if field not in data:
                        missing_fields.append(field)
                    elif not data[field] or (isinstance(data[field], str) and not data[field].strip()):
                        empty_fields.append(field)
                
                # Count missing and empty fields
                for field in missing_fields:
                    analysis['missing_fields'][field] = analysis['missing_fields'].get(field, 0) + 1
                
                for field in empty_fields:
                    analysis['empty_fields'][field] = analysis['empty_fields'].get(field, 0) + 1
                
                if is_complete:
                    analysis['complete_entries'] += 1
                else:
                    analysis['incomplete_entries'] += 1
                    analysis['problematic_entries'].append({
                        'key': key,
                        'missing_fields': missing_fields,
                        'empty_fields': empty_fields
                    })
            
            return analysis
            
        except Exception as e:
            self.logger.error(f"❌ Failed to analyze emoji data: {e}")
            return {}

    def process_emoji_data(self, json_file: str) -> bool:
        """Process emoji data from JSON file and store in vector database"""
        try:
            # Load emoji data
            with open(json_file, 'r', encoding='utf-8') as f:
                emoji_data = json.load(f)
            
            self.logger.info(f"📁 Loaded {len(emoji_data)} emoji entries from {json_file}")
            
            # Create search index
            if not self.create_search_index():
                return False
            
            # Process documents with threading for better performance
            documents = []
            failed_documents = []
            
            with ThreadPoolExecutor(max_workers=5) as executor:
                future_to_key = {
                    executor.submit(self.prepare_document, key, data): key
                    for key, data in emoji_data.items()
                }
                
                for future in as_completed(future_to_key):
                    key = future_to_key[future]
                    try:
                        doc = future.result()
                        if doc:
                            documents.append(doc)
                            self.logger.debug(f"✅ Processed {key}")
                        else:
                            failed_documents.append(key)
                            self.logger.warning(f"⚠️ Failed to process {key}")
                    except Exception as e:
                        failed_documents.append(key)
                        self.logger.error(f"❌ Error processing {key}: {e}")
            
            # Log summary of processing
            self.logger.info(f"📊 Processing summary: {len(documents)} successful, {len(failed_documents)} failed")
            if failed_documents:
                self.logger.warning(f"⚠️ Failed documents: {failed_documents[:10]}{'...' if len(failed_documents) > 10 else ''}")
            
            # Upload documents
            if documents:
                success = self.upload_documents(documents)
                if success:
                    self.logger.info(f"🎉 Successfully processed {len(documents)} emoji entries from {len(emoji_data)} total entries")
                    return True
            
            return False
            
        except Exception as e:
            self.logger.error(f"❌ Failed to process emoji data: {e}")
            return False
        
def main():
    """Main function to demonstrate emoji vector storage"""
    print("🔍 Emoji Vector Storage - Azure AI Search Integration")
    print("=" * 60)
    
    # Check dependencies
    if not AZURE_SEARCH_AVAILABLE:
        print("❌ Azure Search libraries not available.")
        print("Install with: pip install azure-search-documents azure-identity")
        return
    
    if not AZURE_OPENAI_AVAILABLE:
        print("❌ OpenAI library not available.")
        print("Install with: pip install openai")
        return
    
    # Check for emoji data file
    emoji_file = "azure_emoji_emotions.json"
    if not os.path.exists(emoji_file):
        print(f"❌ Emoji data file '{emoji_file}' not found!")
        return
    
    # Check for configuration
    config_file = "azure_config.json"
    if not os.path.exists(config_file):
        print(f"❌ Configuration file '{config_file}' not found!")
        print("Please create azure_config.json with the following structure:")
        print("""
{
    "search_service_name": "your-search-service-name",
    "search_admin_key": "your-search-admin-key",
    "azure_openai_endpoint": "your-azure-openai-endpoint",
    "azure_openai_api_key": "your-azure-openai-api-key",
    "embedding_deployment_name": "text-embedding-ada-002",
    "emoji_index_name": "emoji-emotions"
}
        """)
        return
    
    try:
        # Initialize storage
        storage = EmojiVectorStorage(config_file)
        
        # Analyze emoji data first
        print("\n🔍 Analyzing emoji data quality...")
        analysis = storage.analyze_emoji_data(emoji_file)
        if analysis:
            print(f"📊 Data Analysis Results:")
            print(f"   Total entries: {analysis['total_entries']}")
            print(f"   Complete entries: {analysis['complete_entries']}")
            print(f"   Incomplete entries: {analysis['incomplete_entries']}")
            
            if analysis['missing_fields']:
                print(f"   Most missing fields:")
                for field, count in sorted(analysis['missing_fields'].items(), key=lambda x: x[1], reverse=True)[:5]:
                    print(f"     {field}: {count} entries")
            
            if analysis['empty_fields']:
                print(f"   Most empty fields:")
                for field, count in sorted(analysis['empty_fields'].items(), key=lambda x: x[1], reverse=True)[:5]:
                    print(f"     {field}: {count} entries")
            
            if analysis['problematic_entries']:
                print(f"   First 5 problematic entries:")
                for entry in analysis['problematic_entries'][:5]:
                    print(f"     {entry['key']}: missing={entry['missing_fields']}, empty={entry['empty_fields']}")
        
        # Process and store emoji data
        success = storage.process_emoji_data(emoji_file)
        
        if success:
            print("\n🎉 Vector storage completed successfully!")
            print("\nTry searching for similar emojis:")
            
            # Demo search
            while True:
                query = input("\nEnter search query (or 'quit' to exit): ").strip()
                if query.lower() == 'quit':
                    break
                
                results = storage.search_similar_emojis(query, top_k=3)
                if results:
                    print(f"\n🔍 Similar emojis for '{query}':")
                    for i, result in enumerate(results, 1):
                        print(f"{i}. {result['emoji_name']} ({result['primary_emotion']}) - Score: {result['score']:.3f}")
                else:
                    print("No results found.")
        
    except Exception as e:
        print(f"❌ Error: {e}")


if __name__ == "__main__":
    main()
